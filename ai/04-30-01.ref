==========
Well, we have a great fourth assignment, but it's kind of long, and so it was going to be due on the 15th; but I am not allowed according to the rules of the faculty to have anything due after the 11th. 
__: But you could cancel the final. 
I could. 
I could. 
I debated that, but I decided that I'd better not. 
__: Would you rather have the final? 
Yes. 
__: We're going to have a three-hour final exam? 
Three-hour final, yes. 
But it'll be like the midterm, and everybody did OK on the midterm. 
__: The final will be (inaudible) about your class. 
And you have to stay here, and -- eh. 
Yes, but the final I hope spans the material better. 
The thing is if I had just rearranged the order of the topics a bit. 
I mean I could have actually accommodated the (inaudible) but I didn't (inaudible). 
There's all these things they hand out saying "Rules for Undergraduate Classes," and so I thought, OK, we're not an undergraduate class, but apparently it applies to graduate classes, too. 
And I haven't been here long enough. 
This is the first time I've been in charge of a course, so I didn't know about this. 
__: (inaudible) And I'm told that the students never let you slide about it either, so I wasn't going to be willing to just like (inaudible). 
__: (inaudible) Oh, yes, right. 
The last homework was going to be an interesting implementation thing of learning Bayes' nets, but I can't have it due any later than May 11 by the rules, and I can't give you an interesting problem set to do really between now and May 11, and so ... 
This is OK. 
It's just not so much fun. 
Oh, the clock is back. 
A little bit ahead, but close. 
__: (inaudible) That's the homework. 
__: Are you going to post that, like on the -- Yes, we will. 
And I'll type up the, that you-describe(?) 
problem. 
And I'll also hand out a set of practice problems at some point before the final. 
(pause) I'll wait for a couple more people. 
(pause) All right. 
Let's go. 
OK. 
So, just to let you know where we are in the class and what's happening, there are six more lectures. 
So we're going to have four, really five of them are going to be on learning and then the last one is on philosophy. 
I'll just recap this one more time, but I'll probably do it one more time at the end of the class for the very late comers. 
The story is that I messed up and I originally had the Homework 4 due on the last day of class, but it turns out that I'm not allowed to have the last assignment due in the last week of class, so Homework 4 is going to be shorter than it was intended to be. 
And I'm so sad. 
And so there's Homework 4. So it's going to be a bunch of pencil-and-paper exercises. 
We'll post it on the Web, and in particular I'll type in the parameters of the used car example just in case you weren't taking rigorous notes. 
So the work that remains is going to be this problem set, and then there's going to be a final exam. 
It'll be three hours long. 
It'll be basically like the midterm. 
It will roughly weight -- it will cover all the material we've had in class, but maybe about a third of it will be on the first half and two thirds on the second half to kind of balance out the midterm. 
So, I think that's that. 
OK. 
==========
So, we're going to talk about learning now. 
So, when we started out and we were looking at these deterministic models of inference and problem solving and planning and so on, it seemed plausible that a human could write down the domain descriptions. 
You could say, well, in order to go shopping -- or, you know, in order to buy this thing, you have to go to the store and then you'll get it and so on. 
So you could imagine giving all those kind of operator descriptions, just sort of sitting and writing them down -- although it turns out that in fact people have problems doing that and doing it sort of coherently. 
But writing down deterministic operators is not so hard. 
When we move to probabilistic models of the way the world works and so on, then it becomes a lot less plausible to imagine that you could sit a person down and get them to actually write the models down. 
And maybe you could get them to write the models down, but then you'd have to worry about how good a reflection they would be of what was actually going on in the world. 
So, learning becomes a much more important part of what we're going to do, now that we're thinking about working with probabilistic models of things. 
And, you know, increasingly people realize that, you know, humans know how to do stuff, but it seems to be that the knowledge that they have of how to do things is so kind of hidden and inarticulable that it's really hard to get it out of them anyway. 
I mean you're very good at walking down the hallway, but try writing a program to do it and you'll see that whatever knowledge you have about how to do that is completely inaccessible. 
So, even though humans know how to do the stuff that we're interested in getting, say, robots to do, it seems like a more effective strategy to get the robots to learn how to do it. 
So, we're going to look at the problem of learning, which is to say interacting with the world somehow and figuring out a good way to behave, or figuring out a good way to make different kinds of decisions. 
So that's going to be our theme for the next five lectures. 
There are two kind of high-level strategies -- well, there are a bunch of ways to cut up the space of things that you could do with learning -- but there are two kind of high-level strategies to think about, and we'll actually explore one and then the other. 
So, so far in the probabilistic section of this class, we've looked at sort of probabilistic models and in particular in the form of Bayes' nets. 
And then we spent some time thinking about decision theory and Markov decision processes, which is a way to take a probabilistic model of how the world works and compute the right actions to take. 
So in learning you could do one of two things. 
You could pursue what's sometimes called a generative strategy or to learn a generative model, which means that what you're going to do is learn how the world works -- so, learn a model of the world where it's a probabilistic model of the world. 
And then, given that, use decision theory to choose your actions. 
So, for instance, if you could learn a Bayesian network or learn the chances, the probabilities, of various things happening on the used car lot or those sorts of things -- if you could learn those probabilities, then you could just compute what would be an appropriate action to take. 
So that's one strategy. 
Another strategy is what's sometimes called a discriminative model. 
And in the discriminative model you directly learn what to do. 
So, rather than doing this sort of indirect thing where you learn how the world works and compute how you ought to behave, this strategy says let me just directly learn how I ought to behave or how I ought to make various kinds of decisions. 
So, we're going to talk about this for a couple days, and then we're going to do this next week. 
Mike's going to give a lecture next Monday. 
You guys can all be -- __: That's a (inaudible). 
(laughter) Right. 
Well, right. 
That's good. 
So come and be helpful and supportive and friendly and all that. 
__: And no rotten (inaudible). 
And no rotten (inaudible). 
That's right. 
OK, so we're going to pursue this for a while to see how that goes. 
All right. 
So, how would this work? 
Let's think about what a learning setting would be, and then we'll see how we could do this in some simple cases, and then we'll build our way up to Bayesian networks. 
So, this is sort of a paraphrase of an example in one of the standard pattern recognition books. 
==========
Imagine that you're in a fish canning factory, and the fish come in on a conveyor belt and you have to decide whether they are a salmon or they are a tuna. 
And it turns out that -- I mean, it's going to turn out that, right, because they come by and you look at them and if it's a salmon you shunt it one way, if it's a tuna you shunt it the other way, and then there's some cost for sending them the wrong way. 
So you'd like to send them in the right direction. 
So, what you want to do is eventually make some kind of decision making process to decide salmon versus tuna. 
But you can train, right? 
So you get the fish-expert human in there, not to tell you exactly what the right discrimination rule is, because they can't articulate that, but at least they can point to them and say, "Salmon, salmon, salmon. 
Tuna, tuna. 
Salmon, salmon." 
So you get these labelled examples. 
You get told these are the ones that are salmon and these are the ones that are tuna. 
So what are you going to do? 
Well, presumably, sort of from a probabilistic perspective, you would like to say salmon if - - so let's see -- so you just observe some features of this fish as it comes by on the conveyor belt. 
Maybe you get to see its weight and its length and the color and some other features that are pretty easy to detect, say with a camera and a scale and so on. 
All right. 
And then you're going to say salmon if the probability that it really is a salmon, given the features, is greater than the probability that it's -- no, there's only two kinds of things here. 
(writes) So, if you have a probabilistic model that told you how to look at the features and tell you the probability that it was a salmon -- or look at the features and tell you the probability it was a tuna -- then you could just compare these two things and decide which way to toss the fish. 
So how can we expand this rule out a bit -- because presumably it might be not directly easy to see how to compute that. 
So what can we do, if we have something that says salmon given features? 
We could turn it around using Bayes' rule. 
So let's do that. 
We'll get the probability -- I'm going to (inaudible) -- of features given salmon times the probability of salmon over the probability of the features. 
We'll do the same thing over here -- the probability of features given it's a tuna times the probability of tuna divided by the probability of the features. 
And, so conveniently, this thing that we always like to kind of ignore, this normalized factor, we can really just completely throw out. 
So you can say, well, I'm going to say this thing is a salmon if this is greater than that. 
Now, it turns out that it's actually going to be convenient to write it a somewhat different way. 
I just kind of wanted to show you how this goes. 
So, let's see. 
We could divide everything by this, so we can get the probability of features given that it's a salmon times the probability it's a salmon over the probability of features that it's a tuna times the probability it's a tuna. 
(inaudible). 
So, that's going to be our discrimination rule, and we can use it for other things. 
So this says, again, if only we knew these four quantities then we would know how to build this thing that's going to shunt the fishes in the (inaudible) direction. 
So, these two have to do with the general prevalence of salmon and tuna coming into our factory. 
So those seem like things that would be pretty easy to estimate; you just count. 
But I'll talk a little bit about parameter estimation. 
Sometimes it's a little bit more than counting, but that would be a way to do it. 
So then we're left with this question of the probability of these features given that it's a salmon. 
And what's the probability that we get this particular set of features. 
So the salmon's going to come down there and we're going to get a vector of features, right? 
We're going to get ... 
(writes) 10.2, 11, some RDV(?) 
values -- I don't know. 
We're going to get just some bunch of numbers, and we have to decide how likely is it that we would get that set of readings given that this thing is a salmon. 
OK. 
So there's a couple different -- I mean there's a million different ways you could try to build a model to do this. 
So let's look at a couple. 
The simplest -- well, I don't know if it's the simplest. 
I won't say that, but usually the thing they have first in the textbooks is to treat this ... 
(writes) as a multivariate normal distribution. 
And I know that we never actually required you folks to know anything about continuous statistics, so I'm not really going to talk about this too much; but just I'll do a cartoon, sort of, version of how this goes -- because it's kind of interesting to think about. 
==========
And let's just actually consider the two-dimensional case. 
So let's imagine that we have just height and weight -- not height and weight? 
--well, height and weight essentially -- the weights [or widths?] and length of our fish. 
And we're going to try to use that to determine whether it's a tuna or a salmon. 
So, we can treat them -- this distribution -- we can say, well, let's consider all the salmon in the world. 
What's the joint distribution of their height and weight? 
So you would get a probability distribution -- oh, if we were just looking at their height, it would be some distribution that looked like this, and the length -- you know, this would be the average length. 
In two dimensions it might be -- you'd sort of expect that the longer fish would weigh more and the shorter fish would weigh less, so it might be that the salmon, you would get maybe a probability distribution that looked like that. 
So, if this is the height -- or, I'm sorry. 
I keep using height and weight. 
I meant length and weight. 
(writes) ... 
Then you might get a probability distribution that says, well, with high probability, uh, they tend to be -- they actually tend to be in here but then things sort of decrease out like that. 
So that would be a probability distribution that would tell you, say, the height and weight -- the length and weight -- of the salmon. 
So you could model the probability distribution of these features for the salmon. 
Then you could independently -- how do you do that? 
Well, you go read a book on basic statistics that says how to estimate the mean and the covariance, (inaudible) to a normal distribution. 
It's not so hard. 
You can just do some things -- sum up your data into the sum of the squares and (inaudible) some things -- and you have an estimate of the parameters of this distribution. 
We'll do this a little more carefully in the discrete case, but I wanted to talk about this case because it gives me some nice figures to draw on the board and because it's sort of common(?) 
case. 
OK. 
So you would do that for the salmon. 
(writes) ... 
And then you would do that same thing again for the tuna because you need to build this model that says what's the probability that you get feature vectors of a particular kind for tuna. 
And so you would draw another picture, and I actually have no idea about the relative shape and sizes of salmon and tuna, but imagine that tuna's tend to be sort of -- oh, I don't know -- more heavy for their length, and also not so long, so that they tend to cluster over here or something. 
Then you could build again a distribution that described the shape of a tuna, in general. 
OK. 
So now that you have that, a particular fish comes along the line, you measure 10.2 versus 11. 
So you could look that up in your probability distribution function and say, oh, you know, it's about there and, no, it's about here; and that might tell you, well, gosh, it's very much, apparently, very much less likely to be a tuna than a salmon. 
But you would take these numbers -- the numbers that you got by looking these things up in the distributions -- you would plug them in here. 
But you'd weigh them also according to your prior(?) probability of whether things tend to be tuna or salmon in general, and you would look to see if that number is greater than 1.00 and then you'd pop out the answer or send the fish off in whichever direction it needed to go in. 
So, one thing that's interesting -- I mean this is a perfectly reasonable way to try and address this problem. 
One thing that's interesting is that if you actually take the formal description of the Gaussian distribution and you plug it in here and here and do some algebra and some simplification, you find out some things that are kind of interesting. 
One is that, in general, there is a simple formula for the description of where the cutoff is. 
Right? 
Because if you look at this, we're saying you could look at this whole thing as some function of F. And if G of F is greater than 1.00 ... 
(writes) then we're going to say it's a salmon; if it's less than 1.00 we're going to say it's a tuna. 
So now you could try to look at the general form of this function, G, and it's going to turn out, interestingly enough, that it's a linear -- well, in some cases it's a linear function of the features, and it's always at least a quadratic function of the features. 
So this is a quadratic in the normal case, a quadratic function ... 
(writes) quadratic function of the features. 
What does that mean? 
It means that if you, for instance, it's easier to see this if you -- you could take this density and draw it here, say. 
And now if you compute that function, G, it's going to tell you something like -- it's going to give you a curve, maybe one that looks sort of like that, which says if you're weight- length vector is on, say, this side of the line then say it's a tuna, and if it's on that side of the line say it's a salmon. 
So what's kind of interesting is that you can go through the whole sort of decision theoretic reasoning and so on, but once you do the algebra what pops out is just this simple discrimination that says if you're on this side of the line say one thing and if you're on that side of the line say the other thing. 
So, you might have been going through some complex reasoning off-line but coming out with a very efficient little tester that you can apply on-line. 
And it's just that if these Gaussians are round -- then the separator is actually a line; that's not (inaudible). 
It can be kind of a funny separator though. 
Consider the following case. 
This is one that I always sort of like. 
Imagine that there's one kind of fish that comes along that is incredibly uniform in its features. 
It's not necessarily that it's big or it's small but that there's hardly any variability. 
Maybe you only get ones of a certain age, so they're always just clustered right around there, so that the whole density is just clustered right in there. 
So you know if you get something in here it's pretty surely that, but you're very certain if you get something out here it's not that. 
==========
Imagine you have the other kind of thing that's really extremely variable. 
They're all over the map. 
You never know exactly what you're getting. 
Then you'll get a separator actually that's a circle. 
It'll say, well, if you get something in here then guess that it's that -- the thing that doesn't vary very much; and as soon as you get something out here you'll guess (inaudible). 
So, in general, you have a quadratic separator but the quadratic could be a circle. 
So that's something that we don't(?) 
want. 
So that's an example of what you might do if you have these kinds of continuous features. 
You could get fancier. 
It's a thing we won't really pursue here, but you might say, well, you know, I have these salmon but it turns out that either they're this year's brand-new salmon or they're last year's (inaudible) of adult salmon, and so in fact one Gaussian distribution doesn't model the shapes of the salmon at all because some of them are pretty little and some of them are really big. 
And I need, say, two lumps in my distribution to describe the relative shapes and weights of salmon. 
So that's OK. 
You can move -- you can use any sort of family of probability distributions you want here. 
You could one that's say a mixture of (inaudible). 
You could say some salmon are like this and some are like that. 
That's fine. 
And you can usually make the math go through although the separators start to get more and more complicated. 
OK. 
==========
Let's talk now about the discrete case and something that looks back at what we've been talking about before -- the Bayesian network. 
So, one view that you could have -- we'll call it the naive Bayesian model, and then we'll go to the general one. 
I'll put it in quotations. 
(writes) So, again, you say, well, you're assuming that you're in some domain where there are a bunch of features of these objects that you can observe, and there are some in this case hidden -- or eventually it's going to be a hidden variable that you want to try to specify. 
So, one way to model that is to say, well, my object -- my fish or whatever my things are that I'm trying to classify have some particular class associated with them. 
And as a function of that class there are all these different observable features -- so there's (writes) ... 
Feature 1 and Feature 2 and Feature N, right? 
So this might be the length and the weight and the color and so on. 
But, in order to make my model not too complicated, I'm going to assume that given the class of the thing these features are independent. 
So that the way that we generate a salmon is -- and already you can see that this isn't necessarily such a faithful model but so it's a good starting point. 
So you say, well, -- you can sort of think of this as saying, well, the way salmon are generated or the way fish are generated is that first we pick what kind of fish it's going to be -- either it's a salmon or a tuna -- and then, given that, we independently pick its height and its weight and its so on and so forth. 
Now, what that means is that the height and weight can't be correlated, and so that's what makes this a naive model. 
But let's think about how if we had a model like this we could use it to make decisions. 
And then we'll think about how it is that we could learn the arcs(?) of such a model -- learn the parameters of such a model -- and then we'll think about how to learn a more interesting model. 
So it's going to be (inaudible). 
So this naive Bayesian model, it -- let's see. 
So it's going to give us a particular form for the probability of the features given the class. 
What's that form going to be? 
What's the probability of this whole feature vector given a particular assignment of a class? 
__: (inaudible) Excuse me? 
__: Just the product of the (overlapping dialogue, inaudible). 
It's going to be the product, right. 
So this is going to be the product ... 
(writes) of the probability of Feature I given that it's a salmon ... 
tuna ... 
and then we have the probability of that. 
Right. 
Why is that the product? 
__: (inaudible) Right. 
Because these things are conditionally independent given the class. 
So it's OK just to multiply the probabilities. 
They're not independent in general, but given the class they're assumed to be independent. 
OK. 
Now, if we want to we can ... 
(pause) ... 
It turns out though in this case also -- if you do a little bit of work, which I think it's probably not important to do on the board -- but in this case also if you take -- it turns out, just as a general rule, when you end up with these products over probabilities, what you do to make life easier is take the log of everything. 
If you take the log of everything you get sums(?) and eventually what you can do is arrange for this to come out to be a linear function in the features. 
So, if you have binary features here, for instance, then this will turn out to be a function that's linear in the features. 
So, again, you get a nice, very simple form that tells you -- I mean you could draw the Bayes' net on the board and use that to justify why you're doing what you're doing, but what you might end up with is a very simple decision about how you're going to classify something. 
OK. 
I'm going to talk about one more thing having to do with decision theory, and then we'll talk really about how to learn Bayes' networks. 
The decision theory point is this. 
Imagine that in our tuna cannery -- in our fish cannery -- it's really important that what goes in a can labelled salmon really all be salmon -- right? 
-- because people pay more for that. 
Tuna on the other hand, well, you know, it's sort of -- if people are going to make it into tuna salad it really doesn't matter if it has a little salmon, has a little bit of whatever else in it. 
No one's really going to mind. 
So that you might have a situation in which misclassifications have a different cost, depending on which direction they go. 
Maybe a better motivation for that is -- you know, one place that people use these kinds o learned rules actually for real now is in hospitals. 
So things like trying to decide whether when somebody comes in with chest pains, whether they're having a heart attack or heartburn. 
And that turns out to be a (inaudible). 
Well, presumably, the costs are different for the two kinds of a thing. 
==========
So you would like your model to not just try to minimize the number of mistakes you make, but perhaps take into account how much the different ones cost. 
So we can do that, and let's actually see how that comes out. 
Now, let's say that what I really want to do is make decisions so that I minimize costs. 
And so my cost function is going to be this. 
It's going to be -- so let's say -- so I'm going to talk about some generalized costs of saying tuna. 
(writes) And it's going to be the probability that -- let me write this -- the cost of saying tuna when I observe features F. So that's going to be the probability that it really is a tuna when there is features F, times the cost of saying tuna when it is a tuna -- presumably that's going to be zero, but you could do it in general -- times the probability that it's a salmon given features F, times the cost of saying salmon when it's really a tuna. 
And then similarly I could say what's the cost of saying salmon when I get features F. So it's going to look like this, like it's going to be the probability that it's a tuna when you have features F, times the cost of saying salmon when it's a tuna, plus the probability of ... 
__: On the second one, don't you want the cost of saying it's tuna when it's a salmon? 
Oh, thank you. 
Yes. 
I knew I had to vary one thing, but I varied them both. 
(writes) ... 
And this is a good thing. 
OK. 
So I get something that looks like that. 
And now we're looking at both costs. 
Let's assume that this cost is zero. 
So then we get this term and this term, and so now the idea is that when we get some set of features F, we want to say tuna only if the cost of saying tuna is less than the cost of saying salmon. 
So, when is ... 
(writes) cost of tuna given F less than the cost of saying salmon, and it's going to be just when -- I mean we just multiple, which is kind of cool. 
So we -- before -- well, we'll get a rule that says it's when ... 
(writes) the probability of salmon given F times the cost of tuna when it's really a salmon, when that's less than the probability of tuna ... 
So, before we were just comparing these two probabilities. 
Right? 
We were going to say tuna when in fact the probability of tuna was greater than the probability of salmon. 
So that's right. 
But now we're going to weight our decisions by these relative costs of making the two different kinds of mistakes. 
So, one thing that's really nice, for instance, about learning one of the -- using a generative model is that once you have it you could factor, say, different prompts in. 
Let's say that tomorrow, you know, you learn that the people's preferences changed or that something about the downstream processing in the factory changed. 
You could take those costs and factor them into the model and you just get a direct change in the decision rule that you were using. 
That's one thing that sort of makes decision theory sort of work for us in a useful way. 
OK. 
==========
Now, let's think about how we would learn a Bayesian network. 
So there's two parts to a Bayesian network, right? 
There's the structure and the parameters. 
So first let's talk about learning the parameters ... 
(writes) ... 
so, in particular, the parameters of these conditional probability tables. 
So, somehow, given data, we need to figure out all the conditional probability tables in our Bayes' net. 
So, the data (writes) ... 
is going to be a set of vectors that for - - and somebody's trying to come up with a notation for this -- we'll say with M variables. 
So we have -- oh, let's use V variables, Variable 1 through Variable N. (writes) ... 
So we have K data points, K cases, K fish that someone has told us also what the labels are. 
So we're assuming right now that the data is complete. 
__: (inaudible) M. Oh, yes, because I picked M so it wouldn't conflict with N and then that's N. We're assuming that the data are complete right now. 
That is, every one of these cases that we've seen, every one of these observations, we've seen a value for every variable. 
So, in our fish example, it means we've seen all the attributes of the fish plus the classification of the fish. 
In the other Bayesian networks that we've looked at, it might be, you know, we've observed people driving on icy roads a whole bunch of other times before or something like that. 
Or cases of people coming into the hospital and all their blood pressure and so on and whether they were in fact having a heart attack, all that kind of stuff. 
OK. 
So we're given the data set, and we're given a structure, so we're trying to learn the parameters. 
We assume that the structure is given. 
So our whole problem then is for each node to learn the condi-, to learn the probabilities, to figure out what probabilities go in our conditional probabilities table. 
So let's say I have a simple network like this (writes) ... 
with let's say V1 and V2 and V3. 
So the first parameter I have to learn is the conditional probability table for V1, which is really -- let's say these are binary variables for now because it makes life easy. 
The conditional probability table for V1 is just one number. 
What number is that? 
(inaudible)? 
And it's just the probability of V1. 
It's just the percentage of the time that you know that's true. 
So how would you compute the (inaudible)? 
What do you think? 
What would be a good way to estimate V1, or the number that goes in there? 
Your first guess is almost certainly right. 
(inaudible) data ... 
__: (inaudible) Yes. 
Right. 
Good. 
So on the one hand it could be a prior -- that is, you could get the prior -- you could interview a human and get a prior, but if you have all this data, you could actually go look and you could count -- in my data set here -- how many times did variable V1 have value true? 
That would be one way. 
So you could just count. 
So I'll just write ... 
the number of times V1 is equal to true over, in this case, it would just be the total number of cases -- and that is six. 
It seems like we're being a pessimist. 
Now, let me go off on just a small tangent here. 
This would be the -- in a normal statistics class this would be estimate that you would learn to use and they would tell you that it was the maximum likelihood estimate; and it's good and it has all sorts of useful properties and it means that, you know, if you've seen -- if you have ten cases and four of them were true, then your estimate would be .4, 
and that seems right. 
But there's a danger in using this kind of an estimator and that is zero -- zero and one. 
So, in a Bayesian network -- it's something like this -- when you put a zero somewhere on your conditional probability table -- well, it would be very unusual to end up with a zero here, but you might end up with a zero down in one of these places, in your estimates. 
But to put a zero in a model means you're sure that this can never ever happen. 
It means that, for instance, no matter how big the cost is of making a mistake when that thing is true, you'll completely ignore it because it can't happen. 
Zero means it really, just no matter what, can't happen. 
A Bayesian would rather die than put a zero in their model unless they really -- unless they, you know, they think this thing is less likely than (inaudible). 
All right? 
OK. 
So, zeroes are bad things. 
So you could try to think of ways of getting out of packing the zeroes in your model. 
So you want to maybe make your estimates in such a way that you don't end up with zeroes and ones. 
Well, here's a case where there's like an obvious hack, and the obvious hack has a beautiful mathematical story for why it's a good thing to do, and I don't know which order they came in. 
But here ... 
(writes). 
Just look at that for a minute. 
OK. 
So you know it's never going to be zero, right? 
Or one. 
And it has the property that if you haven't seen any data at all, you'd get a half, which seems like a nice noncommittal value. 
Right? 
So, it's a half in the absence of any data. 
In the limit(?) of infinite data, these things get washed away and it goes towards the estimate that you would have had before. 
So it's a way of kind of hedging your bets a little bit, by starting out saying, you know, I really -- in advance I have no idea what this probability is going to be, and then kind of -- and so your estimate would come out at .5; the more and more data you get, the more and more you push, and you might get pushed and pushed and pushed toward zero but you never actually write a zero down, when means -- very sort of pragmatically -- various things don't explode in your algorithm, and sort of philosophically it means that you never just make an irretrievable commitment (inaudible). 
The story for why this is reasonable I probably won't go into, but just to give you some key words, if you care -- I mean it comes out of Bayesian statistics (writes) ... 
and, in particular, using a beta di-, a beta (inaudible), a beta distribution. 
And the idea is roughly that you start out -- that you, rather than using this data to estimate just one probability value that you think is really the answer here, you're actually -- mentally you have uncertainty, you model the fact that you have uncertainty about what this parameter really is. 
So you say when I start out and I have no data, I think this parameter could be anything between zero and one; I really have no idea what it is. 
And as you get more and more data, you get -- your own kind of mental picture of what this parameter is gets sharper and sharper. 
You'll get a sharper and sharper distribution. 
Anyway, if that doesn't make sense to you, don't worry about it. 
But this is a great hack. 
So you could do it and feel confident that there is a good story for why that's OK. 
All right. 
END OF SIDE Anyway, if it doesn't make sense to you, don't worry about it, but this was a great hat. 
So you can see why I feel confident that there's a good story (inaudible). 
All right, so that's how you would estimate this frame. 
Now, say what goes in this mouth? 
If you get just one part of the probability table, how many entries does it have? 
I could sing the Jeopardy song. 
How many entries on that table? 
Two, yes. 
Probability of V3 given V1 and probability of V3 given not V1. 
Binary. 
And so to estimate this, what do we do? 
We go and we look at our data and we count some things. 
What are we going to count? 
__: (inaudible) Right. 
So number of times V3 is true and V1 is true, plus one, over the number of times V1 is true. 
OK, and that's it. 
Right? 
You can have more nodes and they could have more paths(?) and so on, but fundamentally, once you write down, once you kind of think about what the probabilities are that go in these tables you can just estimate them directly by counting your data. 
==========
So if you're given the problem of a given structure, completely observable data, all you have to do is fill in the parameters, it (inaudible) straightforward. 
And actually, it's a pretty important case, because it turns out that it's a fairly good division of labor in the sense that often humans are good at supplying the general structure of a domain. 
Right? 
These variables affect these other ones and so on, but they're bad at coming up with the probability. 
But if you can then get data, you can use the data to fill in the probability tables and end up with (inaudible). 
So that's a pretty good way to proceed. 
Now let's think about what happens when you don't know the structure and model(?). 
So today I'm going to talk about -- for the rest of the day I'm going to talk about what to do when you don't know the structure and the model, and then next time we'll talk about what to do when some of the parameters are not observable. 
OK. 
So when you don't know the model structure, you have to try to find one. 
How do you do that? 
There's no direct way to do it. 
One thing you could do somehow would be in your -- so let's see what's given. 
What's given is the variables, the number of variables is given and the data is given, and that's all, and what we're trying to find is a whole (inaudible) network, so the structure plus the parameters. 
And we want to find a good one. 
__: So is there a structure that doesn't -- that's not (inaudible) the data? 
Good. 
So that's really an important question. 
The answer can -- no, but there are structures that seem to fit the data better or worse in some sense. 
In fact, that's just about what I'm going to get to here. 
If somebody gives you a model and says, this variable affects these two, it might be that you would look at the data and say, you know, it really doesn't seem like this variable would affect that variable. 
Nonetheless, you could go ahead and estimate those probabilities. 
You could say, well, they told me that this is how the domain works and it must be that I just have not such a good sample but given the data I have, this is the best job I could do, and given the constraint that I have to have that structure, this is the best thing I could do to (inaudible). 
But you're exactly right that there are some structures that don't seem to fit the data very well at all, and so if we give ourselves in some sense the freedom to find a structure, then what we have to do is think about what it means for the structure to fit the data. 
OK, so let's think about -- what we want to do is, given some number of variables and given some data, we want to find the best model of that data, but in order to do that we have to have some measure of what it means to be best, so think about what's going to be our criterion, what's going to make one Bayesian network model of the data better than another Bayesian network model of the data? 
So what are some ideas? 
What would make a model good? 
All right, so there's a kind of a good list of fit notion, right? 
And we can actually -- this is the general idea, but in this case, one way to write it is just to say, well, I would like the probability of my data given the model to be high. 
Right? 
All things considered, I would like to have a model that says my data is really very likely than to have a model that says my data is really very unlikely because that's how I got this data. 
So that would be maybe one criterion. 
Do you have any other criteria? 
__: (inaudible) Small size. 
All right, so let's talk about small size. 
There are a bunch of arguments for why you might want a small model, so first of all, I'm going to digress here, too, because this is an important point, and it comes up over and over again in learning -- in fact, it may be one of the most important points in learning. 
Imagine that ... 
(writes on board) ... 
I give you those points in X, right? 
Here's X and Y. And I ask you to come up with a function, Y as a function of X, to describe those data. 
What kind of a function might we pick? 
A parabola seems like not a bad choice. 
It looks sort of parabola-ly, parabola-related, parabola-ish-like. 
So you fire up MATLAB or something and you say, hey, find me the best parabola and it gives you one. 
And I don't know, that's pretty good, right? 
It kind of goes near the points, with a shape like a parabola, so it gives sort of goodness of fit plus small size over there. 
Well, it's a reasonably good fit to the data, and size here, the way we would measure size, would be maybe that it's second order, and so it's a quadratic function. 
Now, someone might have said, well, I want it for the length(?) of that data. 
And you could ask MATLAB to fit a line to the data, and you'd get something like this; and that would be even simpler, but it would fail fairly badly on the goodness of fit criterion, right? 
We're getting pretty far away from those points. 
On the other hand, you could get it to fit a seventh-order polynomial and -- let's see, one, two, three, four, five, six -- sure. 
No problem. 
And it just might totally nail your points -- by doing that. 
But you wouldn't be very happy about that probably. 
So what's the intuition for why you don't like that seventh-order polynomial model for the data? 
(brief pause) Or maybe you like it? 
Do you like it? 
__: (inaudible) __: (inaudible) You over-fit. 
OK. 
Right. 
So that's a machine learning term. 
But what does that mean? 
__: It doesn't generalize well. 
Good. 
It doesn't generalize well. 
Right. 
So the intuition I think is somehow that you have an idea hidden in your heart of what kind of process generated this data. 
And what it means to fit a curve to some data, at least part of that, is that what you're trying to do is to say something about a generalized process that generated the data or what you'd like to do also is to predict where the next data points might fall. 
That's part of what -- fitting a curve is sort of making a prediction of where other points drawn from the same process might be. 
And your intuition, for whatever reason, tells you that they're going to be more like on the parabola than they're going to be on that pointy(?) 
function. 
The notion of overfitting -- it will probably do, study(?) 
it quantitatively, but the idea is roughly that a solution to a problem is going to be more robust if when you wiggle the points around just a little bit your answer doesn't wiggle around very much. 
Right? 
So if you move these points a little, the line isn't moving hardly at all and the parabola isn't going to change very much either. 
But a little motion in one of these points could change our seventh-order fit really pretty dramatically. 
It might all of a sudden go bloomp! 
And flip all those curves over or something. 
And so, although it wouldn't change the predictions for the points we're looking at, it would have a dramatically different idea of what the answer might be here than it did before. 
So fitting this very high order thing, the notion of overfitting is that it's really working so hard to fit these data points, but if we just moved them a little bit we'd get another crazy solution and it feels like it wouldn't be very robust. 
So there's a whole bunch of terms. 
There's this idea of overfitting. 
In statistics sometimes people talk about bias versus variance. 
And the idea of bias is that you have a strong restriction on the set of things that you could even say, a strong restriction on the set of models you could build, so that the line has high bias. 
On the other hand, variance is this thing I was alluding to before, which is if you change the data points a little bit your answer changes like crazy. 
So on the one hand you don't want to have too much bias because you'd like to have a lot of freedom to represent a lot of different answers. 
On the other hand you'd like not to have too much variance because you don't somehow want your answer to be very dependent on just the tiny little variations in the data that you do get. 
And the sad fact is, of course, that you usually can't have low bias and low variance. 
You have to trade them off against each other. 
And usually the way you do this -- again there's a whole theory of how this goes, but the more data that you have, the more actual points you have, the more complicated a model in some sense you have a license to try to fit. 
So, if you had a hundred thousand points and you tried to fit the seventh-order polynomial you might not get into really so much trouble as you would with six or seven. 
OK. 
So that's -- Yep? 
__: How would you decide whether you wanted to use a parabola or a Gaussian? 
Because from those points it looked like they -- they both have the same number of degrees of freedom and they both look like they'd work. 
You'd have to know something else to know (overlapping dialogue, inaudible)? 
Yes, good. 
That's a really good point. 
So how would you pick between a parabola and a Gaussian? 
The answer is you've got two choices. 
One is prior knowledge -- so you have some idea about -- actually, a parabola versus a Gaussian is a good thing because Gaussians have these big old -- I mean it kind of tails out like this -- so you might have some knowledge about what's going on in the limits of this thing that might help you. 
In the absence of knowledge, if you don't know anything when you come to this problem, there's another thing people do -- I was going to give her a mention; this is a good time to do it -- is something called cross-validation. 
==========
And now we're going to get into a really big can of worms. 
I'll write it over here ... 
(writes) so, cross-validation, or even just validation. 
Actually, the cross part doesn't matter so much. 
What you could do is you could divide up your data ... 
(writes) into two parts, and then on Part 1 of the data you build a quadratic model and you'd also build a Gaussian model. 
Then you would take those two models and you would test them, and you'd look to see, of those two models, which one had a better fit to the data that you didn't use to train(?) them. 
And whichever one fit better to this held-out data, you'd pick that to be your model because you'd say, well, I trained it on part of my data, it did a good job of predicting the rest of it, and so I'm guessing it's going to do a pretty good job of predicting what other future data I might get. 
Yes? 
__: (inaudible) Then you can. 
Right. 
You can recalculate the parameters. 
It turns out that -- I mean this is lovely as a rule of thumb. 
It's actually very hard to argue for on any real formal basis. 
So that means you can do anything you want to. 
But, yes, I mean another thing -- so that's just really validation. 
Another thing people do sometimes is this thing called cross-validation, where people talk, say, about tenfold cross-validation, so what they do is they divide the data up into ten parts and ten different times they train on nine parts and test on one, and they see how well the two models do. 
Then they train on a different nine and test on a different one and see how they do. 
And so then they have, instead of just two different numbers, one score for each model, they'll have ten different numbers and so there's an idea that maybe that reduces the variance with the estimates or something. 
Who really knows? 
So that's one way that you might choose between them. 
The thing is, and this is the can of worms, which is actually sort of interesting. 
==========
There's a theorem called the no-free-lunch theorem ... 
(writes) ... 
which says, basically, that if you come to a machine learning problem with no prior knowledge at all, none at all, then no learning algorithm is better than any other learning algorithm because what you're doing is asking for a miracle. 
You're asking for something to say if these are the answers to these questions what's the answer to that question? 
And in the absence of some knowledge or constraint or something about the domain, who knows? 
It could be anything. 
So the idea that there's one algorithm that's necessarily better than another one is really just nuts, right? 
You can have algorithms that are better fitted to the assumptions that you bring to the problem. 
Like we often have smoothness(?) 
assumptions, or simplicity assumptions, or assumptions about the underlying physical properties that generated the data. 
And those assumptions can sort of lead to algorithms that sort of fit with those assumptions. 
But in the absence of any assumptions at all about what's going on it doesn't matter. 
You can believe(?) 
any algorithm you want to, including the algorithm that always predicts that the answer is seven. 
That's really provably as good as any other algorithm if you don't know anything about the problem. 
So if you ever come across a paper that says this algorithm is always better than this other one for learning, well, don't believe it because they can't make that argument. 
OK, so what we were doing is we were looking at this idea of trading off goodness of fit -- how well do the model and data match up with each other -- against the size of the model. 
So the arguments for having a small size come from a worry about overfitting. 
There's this idea that big models with many parameters in them could get too close to the data and be too sensitive to the particular data points that we have. 
There's another kind of whole story about an expression that looks like this, which is given in a handout that we're not handing out although it was going to be part of the homework assignment that we aren't doing, which shows that you could also try to look at the -- for any of you who have had sort of coding theory -- you could say that what makes a model good is its ability to describe our data and to be small, those two things together, and one way to think about that is to try to find the minimum description length model -- that is, the model that would let us code our data most efficiently but also be small because (inaudible) we have to shove the model down the wires, too. 
So a model describes a probability distribution. 
You can code the data with respect to that probability distribution in some number of bits, and then it's going to take some other number of bits to send the model, and all that together is the score; and what you want to try to do is minimize that. 
And you end up with a decomposition that's the same as this, and then people have sort of religious wars over exactly what this penalty should be like or what are the relative weightings and so on. 
But you always need some kind of setup like this. 
So, let's just adopt a pretty simple version of this ... 
(writes) ... 
so then this can be the number ... 
(writes). 
And by the number of parameters what we mean is the number of entries in the conditional probabilities tables ... 
(writes). 
==========
So we'd like to try to find a model of our data that makes the data as likely as possible, so matches the underlying distribution of the data that we have as well as it can, and that doesn't have too many parameters. 
If we didn't have this term here, what kind of network do you think would give us the best answer to this? 
__: (inaudible) You're on the right track. 
We can't quite have enough for each data point. 
That wouldn't make sense. 
But what's kind of the -- well, I don't know, what's the most complicated model we could have? 
(pause) What allows us to make a Bayes' net model uncomplicated, to not have too many parameters? 
__: (inaudible) Right. 
Number of edges. 
And edges and co-dependence relations. 
Right? 
So, the lack of edges -- I mean, Bayes' net say something by what they don't have, right? 
Their most important assertion is there is no edge here, which says these two things are conditionally independent. 
__: (inaudible) Yes. 
Something like a complete graph, or -- but yes. 
So a network could basically -- in the end encodes no conditional independence assumptions at all. 
And that naturally can get the closest to the data that you have. 
If you really -- that's like sort of the highest order polynomial you're allowed to have, a graph with as many edges in it as you can get. 
Or, actually there are multiple graphs that could potentially encode a network with no independence assumptions, but sort of the idea here is we're assuming, well, deep down there are -- there is some structure in our data -- there really are some independences, and we'd like to try to force the issue. 
We'd like to try to force the network to find some independences, and so we're going to tell it, no, to add another parameter, which is really essentially to add another arc to our network, you're going to have to really get a big gain in how well you model the data, so that you don't add arcs just to kind of do a little better job of modeling (inaudible). 
So that's the fundamental tradeoff. 
OK. 
So that's easy enough to evaluate, right? 
If I give you a Bayes' net structure, you give me how many parameters it is. 
So let's figure out how we're going to evaluate this, the probability of the data given the model. 
OK. 
So, we have this data set, a bunch of individual points; and the assumption -- which actually I didn't articulate but it's important to articulate, and it's important that it be true -- is that the data points are generated independently from the model. 
So the idea is that in the fish cannery those fish are drawn independently from the lake, sort of like balls from an urn. 
And they're not correlated to one another. 
The samples were independent. 
So that's a very strong assumption. 
We assume that the data are drawn independently from some population. 
Now, if the data are drawn independently, it means that the probability of the whole data set is just the product of the probabilities of the individual data points. 
So we can turn this into ... 
(writes) the product of the probability of data point i in the model. 
Ah. 
(inaudible) parameters. 
(writes) And again what people typically do -- because this product is sort of ugly and it's scaled badly and various other things -- is to take a log here. 
So if you're trying to -- and the thing is it's going to interact -- well, you have to be sure that you get the weightings right. 
If you take the log of this, you don't necessarily want to take the log of that. 
You have to think about how, in your application, you want to trade off goodness of fit against size. 
But let's just concentrate on this term for a minute. 
That can come out to be ... 
(writes). 
Ah, OK. 
So the log of that is actually then the sum (inaudible) log of the product of the probability of data i in the model is the sum of the log of the probability of point i in the model, and data point i is an assignment of values to the variables in our Bayesian network. 
So that probability is itself decomposable into products(?), 
right? 
So we can turn this into ... 
(writes) ... 
the sum of the log ... 
of the probability of point k. 
So I used k for the data point, didn't I? I did. 
Sorry for my inconsistency. 
Let me make these changes, and then all will be well. 
No, I used k for number of nodes. 
Darn. 
Well, I'll make this j. 
So now data point j is a whole bunch of -- it's an assignment to every node, and so now we're going to take the product over all the individual nodes of the probability that variable j has whatever value it has in this data point, so Vkj given the values of parents -- I'm just going to write it like that -- the parents of node j in j's k (?). 
So, are you following? 
Remember that you can always write -- to write the probability of the joint distribution in the Bayesian network, it's always the product over all the nodes of the probability that that node has this value given the values of the parents. 
So this is just the fundamental sort of how you articulate the distribution of the Bayes' net. 
So then we can propagate the log over that, and we end up getting a sum over all the data points ... 
(writes) times a sum over all the nodes of the log of the probability of ... 
So, ahuh? 
__: (inaudible) Oh, well, you have to remember we're minimizing and maximizing. 
So I'm maximizing. 
(pause) Yes. 
OK. 
So this is easy, right? 
I mean, given the data set and the Bayes' net that's got all the probabilities assigned to it, we can ask what's the probability that this data was generated by that model. 
And so we buzz through all the cases in the data set, and then we buzz through all the nodes in the network, and we just add up these logged probabilities. 
So what we have seen now is how to compute what's known as the score of a net. 
And we can compute this term, the probability of the data given the model or at least the log of that. 
You can convert back to this if you want to. 
And we can compute the number of parameters. 
And so now we have these two terms and we can trade them off against each other. 
So, for a given network, we can say how good is this network, how good is this network for that data. 
So now we finally have a criterion. 
So, we have a problem that we're trying to solve. 
We know how many variables we're looking for. 
We have a data set. 
We have to define the search for the parameters, and we'd like to find them that maximize this criterion. 
But we don't know how. 
I mean there's no effective way to do it in a sense. 
There's no kind of algorithmic crank to turn and out pops the right answer. 
You could enumerate all the structures, but there are a lot of them, so unless you have a very small problem that's no good. 
So, what do people typically do? 
Well, back to what we were doing early on in class, you search. 
So, the typical thing is to search in structure space ... 
(writes) ... 
So, we start -- we sort of come up with some initial structure for our model, and there are a bunch of ways that you could initialize the search. 
One would be ... 
(writes) you could start with a fully connected network ... 
or you could start with a completely disconnected network ... 
or you could do a random initialization ... 
or one that actually often starts you out I think in a pretty good space is there's an idea that you could compute -- there's a quantity called the mutual information between two random variables. 
It tells you, kind of like correlation, it tells you how much does one (inaudible) the other one. 
And sort of intuitively your mind imagines that you ought to have arcs between variables that have a lot of mutual information -- so, if you could connect up ... 
(writes) high mutual information nodes. 
Note that it's important that this thing not have any cycles(?), 
right? 
One of the fundamental crucial things about Bayes' nets is that they not have cycles. 
So, you have to be sure in your initialization here that somehow you come up with a Bayes' net structure that doesn't have cycles. 
And now you can do local search. 
You can do something that's a lot like MARKSAT(sp?) for instance, or the walk(?) 
part anyway. 
So, what are the things -- to search in the space of structures you could -- so here are the possible moves that you can consider. 
You could ... 
(writes) add an arc somewhere, delete an arc, and it's often useful to also add reverse an arc -- that is to say, just have it point in the other direction. 
Now, reversing an arc is equivalent in some sense to adding and deleting an arc, but it can often be the case that -- you know that there's a dependence between these variables -- it's going to work out better for it to be the other way around. 
If you were to delete it, sort of in preparation to adding it again in the other direction, things would get worse. 
So it can sometimes be hard to delete an arc and add it again in the other direction. 
So the idea is that now what we're going to do is consider these different moves on the graph, see if we can find one that increases the score; if so, take it and look for another move that improves the graph. 
So you can search along greedily in the space of structures. 
And all the tricks that we talked about and that you learned about in the very first assignment having to do with sometimes taking steps that make things worse, or by taking random steps of a certain kind, those things can help in this context also. 
So, fundamentally we've just boiled this problem of learning Bayes' networks down into one of searching in big combinatorial space where we don't necessarily have a very good heuristic function to guide us, and so we just need to try things and see where we go. 
One important thing is that this scoring function can be decomposed in such a way that you don't have to recompute the whole thing over the whole network. 
When you make a simple change, there are some clever ways to sort of restructure the score, so that you can recompute the score of a potential change fairly efficiently, so that it's not so desperately bad every time you consider a move(?); 
you don't have to recompute the score for the entire network. 
So, time is up. 
Let me recap for the -- finally that everyone's here -- that right there is Assignment 4. It's a paper assignment. 
It's due on May 11th. 
I ask that you all write this one out individually. 
So you can talk with each other all you want, but what I mean by write it out individually is that you do it in your own words, that you could have done it at your own desk with no one else in the room, that I could ask you questions about it and you would know what it means. 
And it's due May 11th, which is not one of our normal class days, so it'll be due, let me just say arbitrarily, at 5 p.m. at Mike's office. 
__: Because Mike leaves his office at 5:02. 
Right. 
He's out of there. 
OK. 
See you next class. 
END OF TAPE
==========
